//-- * https://www.sdsc.edu/systems/expanse/user_guide.html
//-- * max running jobs on gpu-shared is 24


// -- * Enable singularity stuff
singularity.enabled     = true
singularity.autoMounts  = true
conda.enabled           = false
docker.enabled          = false
podman.enabled          = false
shifter.enabled         = false
charliecloud.enabled    = false
apptainer.enabled       = false


executor {
    queueSize = 22
}


//-- ! This makes sure that $HOME folder is not being used, and Expanse users won't be angry at you.
workDir = "${params.output_workdir}"



process {
    executor = 'slurm'
    maxRetries = 1
    queue = params.partition
    pollInterval = '2 min'
    queueStatInterval = '5 min'
    submitRateLimit = '6/1min'
    retry.maxAttempts = 1

    clusterOptions = ' --export=ALL --nodes=1 --ntasks-per-node=1 -t 09:00:00 -A $params.project '

    scratch = '/scratch/$USER/job_$SLURM_JOB_ID'

//-- * This makes processes associated with low_cpu label to use only 1 core and 512MB ram

//-- * Will run on XEON GOLD 40 core node with 4 NVIDIA V100, 384 GB RAM, on expanse there are 52 nodes
withLabel:gpu_task {
    cpus = 2
    memory = '16 GB'
    time = '8 h'
    queue = 'gpu-shared'
    clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1   -A ${params.project} --gpus=1"

}

withLabel:cpu_task {
    cpus = 8
    memory = '16 GB'
    time = '8 h'
    queue = 'gpu-shared'
    clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1   -A ${params.project}"

}


withLabel:low_cpu{
    cpus = 4
    memory = '4 GB'
    time = '8 h'
    queue = 'shared'
    clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1 -A ${params.project}"

}





}
timeline.enabled = true
report.enabled = true
report.overwrite = true
